{"id":"scheme-llm-toolkit-00a","title":"Add Hugging Face Inference API provider","description":"Implement Hugging Face Inference API provider.\n\n## Requirements\n- Create src/llm/providers/huggingface.scm with API client\n- Create src/llm/providers/huggingface-provider.scm adapter\n- Support text generation and embeddings\n- Handle both hosted inference and dedicated endpoints\n- Support streaming where available","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:40:18.571117222-05:00","updated_at":"2025-12-21T23:40:18.571117222-05:00","labels":["enhancement","provider"]}
{"id":"scheme-llm-toolkit-2cj","title":"Add exponential backoff retry logic","description":"Implement robust retry logic with exponential backoff for transient API failures.\n\n## Requirements\n- Exponential backoff starting at 1s, max 30s\n- Configurable max retry count (default: 3)\n- Jitter to prevent thundering herd\n- Retry only on retryable errors (429, 5xx)\n\n## Location\n- Add to src/llm/utils/http.scm\n- Apply in provider implementations","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:41:14.05145235-05:00","updated_at":"2025-12-21T23:41:14.05145235-05:00","labels":["enhancement","error-handling"]}
{"id":"scheme-llm-toolkit-9h4","title":"Add practical usage examples and cookbook","description":"Create a cookbook with practical examples for common use cases.\n\n## Examples to Include\n- Simple prompt completion\n- Multi-turn chat conversations\n- Generating embeddings\n- Provider switching at runtime\n- Handling streaming responses\n- Error handling patterns\n- Configuration management\n\n## Format\n- Standalone runnable scripts in examples/ directory\n- Corresponding org-mode documentation","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:41:58.217733712-05:00","updated_at":"2025-12-21T23:41:58.217733712-05:00","labels":["docs","enhancement"]}
{"id":"scheme-llm-toolkit-a0y","title":"Implement function calling / tool use support","description":"Add support for function calling (tool use) in LLM providers.\n\n## Requirements\n- Define tool/function schema in Scheme\n- Send tool definitions with requests\n- Parse tool call responses\n- Support tool execution callbacks\n- Handle multi-turn tool conversations\n\n## Providers\n- OpenAI: function_calling\n- Anthropic: tool_use\n- Ollama: tools (when available)\n\n## Example Usage\n```scheme\n(define-tool get-weather\n  ((city . string))\n  \"Get current weather for a city\")\n\n(llm-chat provider\n  \"What's the weather in Tokyo?\"\n  #:tools (list get-weather))\n```","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-21T23:42:07.11152755-05:00","updated_at":"2025-12-21T23:42:07.11152755-05:00","labels":["enhancement","feature"]}
{"id":"scheme-llm-toolkit-ac3","title":"Implement CLI tool for LLM interactions","description":"Create a command-line interface for interacting with LLM providers.\n\n## Requirements\n- Argument parsing for model, provider, prompt\n- Interactive chat mode with history\n- Batch processing from stdin/files\n- Configuration file support (~/.scheme-llm-toolkit)\n- Output formatting options (plain, json, markdown)\n\n## Commands\n- `scheme-llm chat` - Interactive chat mode\n- `scheme-llm complete \u003cprompt\u003e` - Single completion\n- `scheme-llm embed \u003ctext\u003e` - Generate embeddings\n- `scheme-llm models` - List available models\n\n## Technical Notes\n- Use (ice-9 getopt-long) for argument parsing\n- Store chat history in .scheme-llm-history","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:41:03.744459517-05:00","updated_at":"2025-12-21T23:41:03.744459517-05:00","labels":["cli","enhancement"]}
{"id":"scheme-llm-toolkit-af5","title":"Add Anthropic/Claude provider","description":"Implement Anthropic Claude API provider following the existing provider pattern.\n\n## Requirements\n- Create src/llm/providers/anthropic.scm with API client\n- Create src/llm/providers/anthropic-provider.scm adapter\n- Support chat completions with Messages API\n- Handle authentication via API key\n- Support streaming responses\n\n## Reference\n- Follow patterns from openai-provider.scm\n- Use provider abstraction from core/provider.scm","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-21T23:40:08.702050552-05:00","updated_at":"2025-12-21T23:40:08.702050552-05:00","labels":["enhancement","provider"]}
{"id":"scheme-llm-toolkit-ex6","title":"Add Google Gemini provider","description":"Implement Google Gemini API provider.\n\n## Requirements\n- Create src/llm/providers/gemini.scm with API client\n- Create src/llm/providers/gemini-provider.scm adapter\n- Support generateContent endpoint\n- Handle authentication via API key\n- Support streaming responses","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:40:30.988456528-05:00","updated_at":"2025-12-21T23:40:30.988456528-05:00","labels":["enhancement","provider"]}
{"id":"scheme-llm-toolkit-lik","title":"Add LLM-powered Scheme macro system","description":"Create macros that use LLMs to generate code at compile/expand time.\n\n## Concept\n```scheme\n(define-llm-macro create-validator (description)\n  \"Generate a validation function based on description\")\n\n(create-validator \"validate email addresses\")\n;; Expands to actual validation code\n```\n\n## Considerations\n- Caching generated code\n- Reproducibility concerns\n- Error handling for invalid generations\n- Security implications\n\n## Status\nExperimental feature - explore feasibility first","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T23:42:25.563357918-05:00","updated_at":"2025-12-21T23:42:25.563357918-05:00","labels":["experimental","feature"]}
{"id":"scheme-llm-toolkit-oim","title":"Add embedding similarity and RAG utilities","description":"Implement utilities for working with embeddings and building RAG applications.\n\n## Requirements\n- Cosine similarity function\n- Vector storage interface (in-memory, file-based)\n- Simple retrieval functions\n- Document chunking utilities\n\n## Use Cases\n- Semantic search\n- Document Q\u0026A\n- Context augmentation","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:42:57.94920274-05:00","updated_at":"2025-12-21T23:42:57.94920274-05:00","labels":["enhancement","feature"]}
{"id":"scheme-llm-toolkit-r3m","title":"Add comprehensive test suite with mocking","description":"Create a proper test suite with mocked HTTP responses.\n\n## Requirements\n- Mock HTTP layer to avoid real API calls\n- Test each provider independently\n- Test error handling paths\n- Regression tests for JSON parsing edge cases\n\n## Structure\n```\ntests/\n├── unit/\n│   ├── json-test.scm\n│   ├── http-test.scm\n│   └── provider-test.scm\n├── integration/\n│   ├── ollama-test.scm\n│   └── openai-test.scm\n└── fixtures/\n    └── responses/\n```","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:42:53.394182883-05:00","updated_at":"2025-12-21T23:42:53.394182883-05:00","labels":["infrastructure","testing"]}
{"id":"scheme-llm-toolkit-s9b","title":"Add circuit breaker pattern for API resilience","description":"Implement circuit breaker pattern to handle provider outages gracefully.\n\n## Requirements\n- Track failure rate per provider\n- Open circuit after threshold failures\n- Half-open state for recovery testing\n- Configurable thresholds and timeouts\n\n## Benefits\n- Fail fast when provider is down\n- Prevent cascading failures\n- Allow graceful degradation","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T23:41:20.856080702-05:00","updated_at":"2025-12-21T23:41:20.856080702-05:00","labels":["enhancement","error-handling"]}
{"id":"scheme-llm-toolkit-smu","title":"Add CI/CD pipeline with GitHub Actions","description":"Set up automated testing and quality checks.\n\n## Workflow\n- Run on push and PR\n- Test on Guile 3.0.x\n- Lint Scheme code\n- Run unit tests (mocked)\n- Optional: integration tests with Ollama\n\n## Files\n- .github/workflows/test.yml\n- .github/workflows/release.yml","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:42:55.718375281-05:00","updated_at":"2025-12-21T23:42:55.718375281-05:00","labels":["ci","infrastructure"]}
{"id":"scheme-llm-toolkit-y3m","title":"Add structured output / JSON mode support","description":"Implement structured output support for reliable JSON responses.\n\n## Requirements\n- Support OpenAI's response_format parameter\n- Add json-schema validation for responses\n- Provide Scheme record auto-generation from schema\n- Handle parsing errors gracefully\n\n## Use Cases\n- Extract structured data from text\n- Generate typed responses\n- Build pipelines with predictable outputs","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:42:14.846843385-05:00","updated_at":"2025-12-21T23:42:14.846843385-05:00","labels":["enhancement","feature"]}
