{"id":"scheme-llm-toolkit-00a","title":"Add Hugging Face Inference API provider","description":"Implement Hugging Face Inference API provider.\n\n## Requirements\n- Create src/llm/providers/huggingface.scm with API client\n- Create src/llm/providers/huggingface-provider.scm adapter\n- Support text generation and embeddings\n- Handle both hosted inference and dedicated endpoints\n- Support streaming where available","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:40:18.571117222-05:00","updated_at":"2025-12-21T23:40:18.571117222-05:00","labels":["enhancement","provider"]}
{"id":"scheme-llm-toolkit-0gl","title":"Add response caching system","description":"Implement caching for LLM responses.\n\n## Requirements\n- Content-addressable cache (hash of prompt + params)\n- Configurable TTL\n- Cache invalidation strategies\n- Persistent storage option\n- Memory and disk backends\n\n## Benefits\n- Reduce API costs\n- Faster repeated queries\n- Reproducible results","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T23:49:16.297857372-05:00","updated_at":"2025-12-21T23:49:16.297857372-05:00","labels":["feature","performance"]}
{"id":"scheme-llm-toolkit-2cj","title":"Add exponential backoff retry logic","description":"Implement robust retry logic with exponential backoff for transient API failures.\n\n## Requirements\n- Exponential backoff starting at 1s, max 30s\n- Configurable max retry count (default: 3)\n- Jitter to prevent thundering herd\n- Retry only on retryable errors (429, 5xx)\n\n## Location\n- Add to src/llm/utils/http.scm\n- Apply in provider implementations","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:41:14.05145235-05:00","updated_at":"2025-12-21T23:41:14.05145235-05:00","labels":["enhancement","error-handling"]}
{"id":"scheme-llm-toolkit-2ge","title":"Add schema validation for responses","description":"Implement JSON schema validation for LLM responses.\n\n## Requirements\n- Define expected response schemas\n- Validate responses against schemas\n- Auto-retry on validation failure\n- Generate Scheme records from schemas\n\n## Integrates with\n- Structured output support (y3m)\n- Function calling (a0y)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:49:41.815490358-05:00","updated_at":"2025-12-21T23:49:41.815490358-05:00","labels":["feature","type-safety"]}
{"id":"scheme-llm-toolkit-3zh","title":"Add contract system for type safety","description":"Implement design-by-contract for API boundaries.\n\n## Requirements\n- Pre/post conditions for functions\n- Runtime contract checking\n- Clear error messages on violation\n- Optional compile-time checks\n\n## Example\n```scheme\n(define/contract (llm-complete provider prompt)\n  (-\u003e provider? string? string?)\n  ...)\n```","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T23:49:22.169868378-05:00","updated_at":"2025-12-21T23:49:22.169868378-05:00","labels":["feature","type-safety"]}
{"id":"scheme-llm-toolkit-8kd","title":"Add API documentation generation","description":"Generate API documentation from source code.\n\n## Requirements\n- Extract docstrings from modules\n- Generate markdown/HTML docs\n- Include usage examples\n- Document all public APIs\n\n## Output\n- docs/api/index.md\n- Per-module documentation","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:49:48.953040655-05:00","updated_at":"2025-12-21T23:49:48.953040655-05:00","labels":["docs","tooling"]}
{"id":"scheme-llm-toolkit-9h4","title":"Add practical usage examples and cookbook","description":"Create a cookbook with practical examples for common use cases.\n\n## Examples to Include\n- Simple prompt completion\n- Multi-turn chat conversations\n- Generating embeddings\n- Provider switching at runtime\n- Handling streaming responses\n- Error handling patterns\n- Configuration management\n\n## Format\n- Standalone runnable scripts in examples/ directory\n- Corresponding org-mode documentation","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:41:58.217733712-05:00","updated_at":"2025-12-21T23:41:58.217733712-05:00","labels":["docs","enhancement"]}
{"id":"scheme-llm-toolkit-a0y","title":"Implement function calling / tool use support","description":"Add support for function calling (tool use) in LLM providers.\n\n## Requirements\n- Define tool/function schema in Scheme\n- Send tool definitions with requests\n- Parse tool call responses\n- Support tool execution callbacks\n- Handle multi-turn tool conversations\n\n## Providers\n- OpenAI: function_calling\n- Anthropic: tool_use\n- Ollama: tools (when available)\n\n## Example Usage\n```scheme\n(define-tool get-weather\n  ((city . string))\n  \"Get current weather for a city\")\n\n(llm-chat provider\n  \"What's the weather in Tokyo?\"\n  #:tools (list get-weather))\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T23:42:07.11152755-05:00","updated_at":"2025-12-21T23:59:35.799628269-05:00","closed_at":"2025-12-21T23:59:35.799628269-05:00","close_reason":"Implemented function calling with tool abstraction for OpenAI and Anthropic","labels":["enhancement","feature"]}
{"id":"scheme-llm-toolkit-ac3","title":"Implement CLI tool for LLM interactions","description":"Create a command-line interface for interacting with LLM providers.\n\n## Requirements\n- Argument parsing for model, provider, prompt\n- Interactive chat mode with history\n- Batch processing from stdin/files\n- Configuration file support (~/.scheme-llm-toolkit)\n- Output formatting options (plain, json, markdown)\n\n## Commands\n- `scheme-llm chat` - Interactive chat mode\n- `scheme-llm complete \u003cprompt\u003e` - Single completion\n- `scheme-llm embed \u003ctext\u003e` - Generate embeddings\n- `scheme-llm models` - List available models\n\n## Technical Notes\n- Use (ice-9 getopt-long) for argument parsing\n- Store chat history in .scheme-llm-history","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:41:03.744459517-05:00","updated_at":"2025-12-21T23:41:03.744459517-05:00","labels":["cli","enhancement"]}
{"id":"scheme-llm-toolkit-af1","title":"Add prompt template system","description":"Implement a template system for reusable prompts.\n\n## Requirements\n- Named template definitions\n- Parameter placeholders\n- Template inheritance\n- Conditional sections\n\n## Example\n```scheme\n(define-template summarize\n  #:params (text max-length)\n  \"Summarize the following in {{max-length}} words:\\n{{text}}\")\n```","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:48:55.374312171-05:00","updated_at":"2025-12-21T23:48:55.374312171-05:00","labels":["feature","prompt-engineering"]}
{"id":"scheme-llm-toolkit-af5","title":"Add Anthropic/Claude provider","description":"Implement Anthropic Claude API provider following the existing provider pattern.\n\n## Requirements\n- Create src/llm/providers/anthropic.scm with API client\n- Create src/llm/providers/anthropic-provider.scm adapter\n- Support chat completions with Messages API\n- Handle authentication via API key\n- Support streaming responses\n\n## Reference\n- Follow patterns from openai-provider.scm\n- Use provider abstraction from core/provider.scm","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T23:40:08.702050552-05:00","updated_at":"2025-12-21T23:54:51.623879476-05:00","closed_at":"2025-12-21T23:54:51.623879476-05:00","close_reason":"Implemented Anthropic Claude provider with Messages API support","labels":["enhancement","provider"]}
{"id":"scheme-llm-toolkit-c0z","title":"Add plugin architecture","description":"Design extensible plugin system for custom providers and features.\n\n## Requirements\n- Plugin discovery mechanism\n- Standard plugin interface\n- Lifecycle hooks (init, shutdown)\n- Configuration per plugin\n\n## Use Cases\n- Custom LLM providers\n- Response transformers\n- Logging/monitoring plugins\n- Custom prompt processors","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T23:50:02.450248382-05:00","updated_at":"2025-12-21T23:50:02.450248382-05:00","labels":["architecture","feature"]}
{"id":"scheme-llm-toolkit-ex6","title":"Add Google Gemini provider","description":"Implement Google Gemini API provider.\n\n## Requirements\n- Create src/llm/providers/gemini.scm with API client\n- Create src/llm/providers/gemini-provider.scm adapter\n- Support generateContent endpoint\n- Handle authentication via API key\n- Support streaming responses","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:40:30.988456528-05:00","updated_at":"2025-12-21T23:40:30.988456528-05:00","labels":["enhancement","provider"]}
{"id":"scheme-llm-toolkit-h7w","title":"Add batch processing support","description":"Implement batch processing for multiple prompts.\n\n## Requirements\n- Parallel request execution\n- Rate limiting\n- Progress tracking\n- Result aggregation\n- Error handling per item\n\n## Use Cases\n- Processing document collections\n- Bulk translations\n- Dataset augmentation","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T23:49:10.424817376-05:00","updated_at":"2025-12-21T23:49:10.424817376-05:00","labels":["feature","performance"]}
{"id":"scheme-llm-toolkit-lik","title":"Add LLM-powered Scheme macro system","description":"Create macros that use LLMs to generate code at compile/expand time.\n\n## Concept\n```scheme\n(define-llm-macro create-validator (description)\n  \"Generate a validation function based on description\")\n\n(create-validator \"validate email addresses\")\n;; Expands to actual validation code\n```\n\n## Considerations\n- Caching generated code\n- Reproducibility concerns\n- Error handling for invalid generations\n- Security implications\n\n## Status\nExperimental feature - explore feasibility first","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T23:42:25.563357918-05:00","updated_at":"2025-12-21T23:42:25.563357918-05:00","labels":["experimental","feature"]}
{"id":"scheme-llm-toolkit-oim","title":"Add embedding similarity and RAG utilities","description":"Implement utilities for working with embeddings and building RAG applications.\n\n## Requirements\n- Cosine similarity function\n- Vector storage interface (in-memory, file-based)\n- Simple retrieval functions\n- Document chunking utilities\n\n## Use Cases\n- Semantic search\n- Document Q\u0026A\n- Context augmentation","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:42:57.94920274-05:00","updated_at":"2025-12-21T23:42:57.94920274-05:00","labels":["enhancement","feature"]}
{"id":"scheme-llm-toolkit-r3m","title":"Add comprehensive test suite with mocking","description":"Create a proper test suite with mocked HTTP responses.\n\n## Requirements\n- Mock HTTP layer to avoid real API calls\n- Test each provider independently\n- Test error handling paths\n- Regression tests for JSON parsing edge cases\n\n## Structure\n```\ntests/\n├── unit/\n│   ├── json-test.scm\n│   ├── http-test.scm\n│   └── provider-test.scm\n├── integration/\n│   ├── ollama-test.scm\n│   └── openai-test.scm\n└── fixtures/\n    └── responses/\n```","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:42:53.394182883-05:00","updated_at":"2025-12-21T23:42:53.394182883-05:00","labels":["infrastructure","testing"]}
{"id":"scheme-llm-toolkit-ryv","title":"Add conversation management","description":"Implement conversation state management for multi-turn chats.\n\n## Requirements\n- Conversation history storage\n- Context window management (truncation strategies)\n- Session persistence\n- Message role tracking\n\n## Example\n```scheme\n(define conv (make-conversation))\n(conversation-add! conv 'user \"Hello\")\n(conversation-add! conv 'assistant \"Hi there!\")\n(conversation-messages conv)  ; =\u003e list of messages\n```","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:49:01.870538978-05:00","updated_at":"2025-12-21T23:49:01.870538978-05:00","labels":["core","feature"]}
{"id":"scheme-llm-toolkit-s9b","title":"Add circuit breaker pattern for API resilience","description":"Implement circuit breaker pattern to handle provider outages gracefully.\n\n## Requirements\n- Track failure rate per provider\n- Open circuit after threshold failures\n- Half-open state for recovery testing\n- Configurable thresholds and timeouts\n\n## Benefits\n- Fail fast when provider is down\n- Prevent cascading failures\n- Allow graceful degradation","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T23:41:20.856080702-05:00","updated_at":"2025-12-21T23:41:20.856080702-05:00","labels":["enhancement","error-handling"]}
{"id":"scheme-llm-toolkit-smu","title":"Add CI/CD pipeline with GitHub Actions","description":"Set up automated testing and quality checks.\n\n## Workflow\n- Run on push and PR\n- Test on Guile 3.0.x\n- Lint Scheme code\n- Run unit tests (mocked)\n- Optional: integration tests with Ollama\n\n## Files\n- .github/workflows/test.yml\n- .github/workflows/release.yml","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:42:55.718375281-05:00","updated_at":"2025-12-21T23:42:55.718375281-05:00","labels":["ci","infrastructure"]}
{"id":"scheme-llm-toolkit-upj","title":"Implement S-expression prompt DSL","description":"Create a domain-specific language for prompt construction using S-expressions.\n\n## Requirements\n- Define prompt primitives (system, user, assistant messages)\n- Support nested composition\n- Enable variable interpolation\n- Provide macros for common patterns\n\n## Example\n```scheme\n(define-prompt code-review\n  (system \"You are a code reviewer\")\n  (user (template \"Review this code:\\n\" (var 'code))))\n```\n\n## From Roadmap\nPart of Prompt Engineering section","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:48:45.950870093-05:00","updated_at":"2025-12-21T23:48:45.950870093-05:00","labels":["feature","prompt-engineering"]}
{"id":"scheme-llm-toolkit-wjs","title":"Add provider setup guides","description":"Create step-by-step setup guides for each provider.\n\n## Guides Needed\n- Ollama local setup (Linux, macOS, FreeBSD)\n- OpenAI API key configuration\n- Anthropic API setup\n- HuggingFace token configuration\n\n## Include\n- Prerequisites\n- Configuration examples\n- Troubleshooting common issues","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:49:56.209023611-05:00","updated_at":"2025-12-21T23:49:56.209023611-05:00","labels":["docs"]}
{"id":"scheme-llm-toolkit-y3m","title":"Add structured output / JSON mode support","description":"Implement structured output support for reliable JSON responses.\n\n## Requirements\n- Support OpenAI's response_format parameter\n- Add json-schema validation for responses\n- Provide Scheme record auto-generation from schema\n- Handle parsing errors gracefully\n\n## Use Cases\n- Extract structured data from text\n- Generate typed responses\n- Build pipelines with predictable outputs","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T23:42:14.846843385-05:00","updated_at":"2025-12-21T23:42:14.846843385-05:00","labels":["enhancement","feature"]}
